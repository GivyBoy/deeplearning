{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT Walkthrough\n",
    "\n",
    "1. Patch Embedding\n",
    "2. Self-Attention\n",
    "3. Feed Forward Network\n",
    "4. Transformer Block\n",
    "5. ViT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchsummary import summary\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"porsche918.jpg\")\n",
    "transform = Compose([Resize((224, 224)), ToTensor()])\n",
    "img = transform(img).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch Embedding\n",
    "\n",
    "- in order to handle 2D imgs, we need to convert them from `HxWxC` to `Nx(P^2*C)`, where `HxW` is the height and width of the image, `C` is the number of channels, `(P,P)` is the resolution of the patches, and `N = HW/P^2` (the number of patches in the image).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 16\n",
    "b, c, h, w = img.shape\n",
    "patch_dim = patch_size * patch_size * c\n",
    "num_patches = (h // patch_size) ** 2\n",
    "embed_size = 768\n",
    "patches = nn.Sequential(\n",
    "    nn.LayerNorm(h),\n",
    "    Rearrange(\"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1=patch_size, p2=patch_size),\n",
    "    nn.Linear(patch_dim, embed_size),\n",
    "    nn.LayerNorm(embed_size),\n",
    ")\n",
    "\n",
    "cls_token = nn.Parameter(torch.randn(embed_size))\n",
    "pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, embed_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_patches = patches(img)\n",
    "img_patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_token = repeat(cls_token, \"d -> b 1 d\", b=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepend the cls_token to the patches\n",
    "img_patches = torch.cat([cls_token, img_patches], dim=1)\n",
    "img_patches.shape  # now we see why we added 1 to the num_patches, when initializing pos_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_patches += pos_embed\n",
    "img_patches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that completes the patch_embedding section of the vision transformer! now, let's put it all together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 3,\n",
    "        patch_size: int = 16,\n",
    "        emb_size: int = 768,\n",
    "        img_size: int = 224,\n",
    "        dropout: float = 1e-3,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        patch_dim = patch_size**2 * in_channels  # each patch is patch_size x patch_size x in_channels\n",
    "        num_patches = (\n",
    "            img_size // patch_size\n",
    "        ) ** 2  # (h // patch_size) * (w // patch_size) -> (img_size // patch_size) * (img_size // patch_size) ** 2 since h = w\n",
    "\n",
    "        self.patch_embedding = nn.Sequential(\n",
    "            nn.LayerNorm(img_size),\n",
    "            Rearrange(\"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1=patch_size, p2=patch_size),\n",
    "            nn.Linear(patch_dim, emb_size),\n",
    "            nn.LayerNorm(emb_size),\n",
    "        )\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(emb_size))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, emb_size))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, c, h, w = x.shape\n",
    "        x = self.patch_embedding(x)\n",
    "        cls_tokens = repeat(self.cls_token, \"d -> b 1 d\", b=batch_size)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        x += self.pos_embedding\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PatchEmbedding()(img).shape  # same shape as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultiHead Self-Attention (MHA)\n",
    "\n",
    "- this is where the magic happens!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 8\n",
    "queries = nn.Linear(embed_size, embed_size)\n",
    "keys = nn.Linear(embed_size, embed_size)\n",
    "values = nn.Linear(embed_size, embed_size)\n",
    "proj = nn.Linear(embed_size, embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries(img_patches).shape  # same shape for the keys and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = rearrange(queries(img_patches), \"b n (h d) -> b h n d\", h=num_heads)\n",
    "keys = rearrange(keys(img_patches), \"b n (h d) -> b h n d\", h=num_heads)\n",
    "values = rearrange(values(img_patches), \"b n (h d) -> b h n d\", h=num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the dot product between the queries and keys\n",
    "scores = torch.einsum(\"bhad, bhcd -> bhac\", queries, keys)\n",
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores /= embed_size ** (1 / 2)  # think of this as some sort of normalization\n",
    "attn = F.softmax(\n",
    "    scores, dim=-1\n",
    ")  # the glorious attention! softmax ensures that the values are between 0 and 1 and sum to 1\n",
    "attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply attn to the values matrix\n",
    "out = torch.einsum(\"bhad, bhdc -> bhac\", attn, values)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lastly, apply the projection\n",
    "out = proj(out)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "again, you know the drill, let's put it together!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size: int = 768, num_heads: int = 8, dropout: float = 1e-3) -> None:\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_size = embed_size\n",
    "        self.query = nn.Linear(embed_size, embed_size)\n",
    "        self.key = nn.Linear(embed_size, embed_size)\n",
    "        self.value = nn.Linear(embed_size, embed_size)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        queries = rearrange(self.query(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        keys = rearrange(self.key(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        values = rearrange(self.value(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        # dot product between queries and keys\n",
    "        scores = torch.einsum(\"bhad, bhcd -> bhac\", queries, keys)\n",
    "        scores /= self.embed_size ** (1 / 2)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "        # dot prod between attn scores and the values\n",
    "        out = torch.einsum(\"bhad, bhdc -> bhac\", attn, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiHeadAttention()(img_patches).shape  # nice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed Forward Network (FFN)\n",
    "\n",
    "- this is allows the model to learn more about the features of the image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expansion = 4\n",
    "dropout = 1e-3\n",
    "ffn = nn.Sequential(\n",
    "    nn.Linear(embed_size, embed_size * expansion),\n",
    "    nn.GELU(),\n",
    "    nn.Dropout(dropout),\n",
    "    nn.Linear(embed_size * expansion, embed_size),\n",
    "    nn.Dropout(dropout),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = ffn(out)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you know the drill!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, embed_size: int = 768, expansion: int = 4, dropout: float = 1e-3) -> None:\n",
    "        super().__init__()\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_size, embed_size * expansion),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_size * expansion, embed_size),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.ffn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFN()(out).shape  # nice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Block\n",
    "\n",
    "- let's put it all (well, most) together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size: int = 768, num_heads: int = 8, dropout: float = 1e-3) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = MultiHeadAttention(embed_size, num_heads, dropout)\n",
    "        self.ffn = FFN(embed_size, 4, dropout)\n",
    "        self.ffn_norm = nn.LayerNorm(embed_size)\n",
    "        self.attn_norm = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.attn(self.attn_norm(x))\n",
    "        x = x + self.ffn(self.ffn_norm(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TransformerBlock()(img_patches).shape  # nice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ViT\n",
    "\n",
    "- lets _actually_ put it together now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 3,\n",
    "        patch_size: int = 16,\n",
    "        img_size: int = 224,\n",
    "        embed_size: int = 768,\n",
    "        num_heads: int = 8,\n",
    "        depth: int = 12,\n",
    "        num_classes: int = 1000,\n",
    "        dropout: float = 1e-3,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embed = PatchEmbedding(in_channels, patch_size, embed_size, img_size, dropout)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(embed_size, num_heads, dropout) for _ in range(depth)])\n",
    "        self.fc = nn.Linear(embed_size, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.patch_embed(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ViT()(img).shape  # nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ViT().to(device)\n",
    "summary(ViT(), (3, 224, 224), device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
