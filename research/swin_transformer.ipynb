{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchsummary import summary\n",
    "from einops import rearrange\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"porsche918.jpg\")\n",
    "transform = Compose([Resize((224, 224)), ToTensor()])\n",
    "img = transform(img).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    in_channels: int = 3\n",
    "    patch_size: int = 4\n",
    "    embed_dim: int = 96\n",
    "    window_size: int = 7\n",
    "    n_heads: int = 6\n",
    "    mask: bool = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swin-Transformer\n",
    "\n",
    "1. Patches + Embedding\n",
    "2. Patch Merging\n",
    "3. Shifted Window Attention\n",
    "4. Relative Position Embedding\n",
    "5. Encoder\n",
    "6. Swin-Transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patches + Embedding\n",
    "\n",
    "- similar to ViT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinEmbeddings(nn.Module):\n",
    "    def __init__(self, config: dataclass = Config) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_embedding = nn.Conv2d(\n",
    "            config.in_channels, config.embed_dim, kernel_size=config.patch_size, stride=config.patch_size\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(config.embed_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear_embedding(x)\n",
    "        x = rearrange(x, \"b c h w -> b (h w) c\")\n",
    "        x = self.relu(self.norm(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3136, 96])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SwinEmbeddings()(img).shape  # correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch Merging\n",
    "\n",
    "- the number of tokens is reduced by these layers as the network gets deeper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "    def __init__(self, config: dataclass = Config) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4 * config.embed_dim, 2 * config.embed_dim)\n",
    "        self.ln = nn.LayerNorm(2 * config.embed_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        h = w = int((x.shape[1] ** (1 / 2)) / 2)\n",
    "        x = rearrange(x, \"b (h s1 w s2) c -> b (h w) (s1 s2 c)\", h=h, w=w, s1=2, s2=2)\n",
    "        x = self.ln(self.linear(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 784, 192])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PatchMerging()(SwinEmbeddings()(img)).shape  # correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shifted Window Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShiftedWindowMSA(nn.Module):\n",
    "    def __init__(self, config: dataclass = Config) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.proj1 = nn.Linear(2 * self.config.embed_dim, self.config.embed_dim * 3)\n",
    "        self.proj2 = nn.Linear(self.config.embed_dim, self.config.embed_dim * 2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        head_dim = self.config.embed_dim // self.config.n_heads\n",
    "        h = w = int(x.shape[1] ** (1 / 2))\n",
    "        x = self.proj1(x)\n",
    "\n",
    "        x = rearrange(x, \"b (h w) (c k) -> b h w c k\", h=h, w=w, k=3)\n",
    "\n",
    "        if self.config.mask:\n",
    "            x = torch.roll(x, (-self.config.window_size // 2, -self.config.window_size // 2), dims=(1, 2))\n",
    "\n",
    "        x = rearrange(\n",
    "            x,\n",
    "            \"b (h m1) (w m2) (H e) k -> b H h w (m1 m2) e k\",\n",
    "            H=self.config.n_heads,\n",
    "            h=h // self.config.window_size,\n",
    "            m1=self.config.window_size,\n",
    "            m2=self.config.window_size,\n",
    "        )\n",
    "\n",
    "        q, k, v = x.chunk(3, dim=-1)\n",
    "        q, k, v = q.squeeze(-1), k.squeeze(-1), v.squeeze(-1)\n",
    "        attn_scores = torch.matmul(q, k.transpose(-1, -2)) / (head_dim**0.5)\n",
    "\n",
    "        if self.config.mask:\n",
    "            row_mask = torch.zeros((self.config.window_size**2, self.config.window_size**2))\n",
    "            row_mask[\n",
    "                -self.config.window_size * (self.config.window_size // 2) :,\n",
    "                0 : -self.config.window_size * (self.config.window_size // 2),\n",
    "            ] = float(\"-inf\")\n",
    "            row_mask[\n",
    "                0 : -self.config.window_size * (self.config.window_size // 2),\n",
    "                -self.config.window_size * (self.config.window_size // 2) :,\n",
    "            ] = float(\"-inf\")\n",
    "            column_mask = rearrange(\n",
    "                row_mask, \"(r w1) (c w2) -> (w1 r) (w2 c)\", w1=self.config.window_size, w2=self.config.window_size\n",
    "            )\n",
    "            attn_scores[:, :, -1, :] += row_mask\n",
    "            attn_scores[:, :, :, -1] += column_mask\n",
    "\n",
    "        attn = F.softmax(attn_scores, dim=-1) @ v\n",
    "        out = rearrange(\n",
    "            attn, \"b H h w (m1 m2) e -> b (h m1) (w m2) (H e)\", m1=self.config.window_size, m2=self.config.window_size\n",
    "        )\n",
    "        out = rearrange(out, \"b h w c -> b (h w) c\")\n",
    "        return self.proj2(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 784, 192])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ShiftedWindowMSA()(PatchMerging()(SwinEmbeddings()(img))).shape  # correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
